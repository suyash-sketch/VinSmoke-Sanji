{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TRAINIG SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SUYASH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.9108 - loss: 0.1862 - val_accuracy: 0.9994 - val_loss: 0.0021\n",
      "Epoch 2/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 34ms/step - accuracy: 0.9984 - loss: 0.0052 - val_accuracy: 0.9992 - val_loss: 0.0022\n",
      "Epoch 3/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 37ms/step - accuracy: 0.9998 - loss: 6.9566e-04 - val_accuracy: 0.9989 - val_loss: 0.0049\n",
      "Epoch 4/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9998 - loss: 7.8397e-04 - val_accuracy: 0.9971 - val_loss: 0.0158\n",
      "Epoch 5/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 0.9994 - val_loss: 0.0015\n",
      "Epoch 6/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.4836e-04 - val_accuracy: 0.9994 - val_loss: 0.0019\n",
      "Epoch 7/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 8.4340e-05 - val_accuracy: 0.9993 - val_loss: 0.0018\n",
      "Epoch 8/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 36ms/step - accuracy: 0.9998 - loss: 0.0015 - val_accuracy: 0.9992 - val_loss: 0.0044\n",
      "Epoch 9/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 34ms/step - accuracy: 0.9994 - loss: 0.0029 - val_accuracy: 0.9992 - val_loss: 0.0037\n",
      "Epoch 10/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 35ms/step - accuracy: 0.9998 - loss: 0.0021 - val_accuracy: 0.9993 - val_loss: 0.0025\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.9995 - loss: 0.0045\n",
      "\n",
      "Test Loss: 0.0108\n",
      "Test Accuracy: 0.9990\n",
      "Tokenizer saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "class FakeNewsDetector:\n",
    "    def __init__(self, max_features=5000, max_length=500):\n",
    "        \"\"\"Initialize Fake News Detection model\"\"\"\n",
    "        self.max_features = max_features\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = Tokenizer(num_words=max_features)\n",
    "        self.model = None\n",
    "    \n",
    "    def load_datasets(self, fake_csv, true_csv, text_column='text'):\n",
    "        \"\"\"\n",
    "        Load and preprocess datasets from CSV files\n",
    "        \"\"\"\n",
    "        try:\n",
    "            fake_df = pd.read_csv(fake_csv)\n",
    "            true_df = pd.read_csv(true_csv)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV files: {e}\")\n",
    "            raise\n",
    "        \n",
    "        if text_column not in fake_df.columns or text_column not in true_df.columns:\n",
    "            raise ValueError(f\"Text column '{text_column}' not found in one or both datasets\")\n",
    "        \n",
    "        fake_texts = fake_df[text_column].fillna('').astype(str).tolist()\n",
    "        true_texts = true_df[text_column].fillna('').astype(str).tolist()\n",
    "        \n",
    "        texts = fake_texts + true_texts\n",
    "        labels = [1] * len(fake_texts) + [0] * len(true_texts)\n",
    "        \n",
    "        return texts, np.array(labels)\n",
    "    \n",
    "    def preprocess_text(self, texts):\n",
    "        \"\"\"Preprocess text data by tokenizing and padding\"\"\"\n",
    "        texts = [text.lower() for text in texts]\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        return pad_sequences(sequences, maxlen=self.max_length, truncating='post', padding='post')\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"Build CNN model for text classification\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_features, 128, input_length=input_shape[1]),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def train(self, fake_csv, true_csv, text_column='text', test_size=0.2, epochs=10, batch_size=32):\n",
    "        \"\"\"Train the fake news detection model from CSV files\"\"\"\n",
    "        X, y = self.load_datasets(fake_csv, true_csv, text_column)\n",
    "        X_processed = self.preprocess_text(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_processed, y, test_size=test_size, random_state=42)\n",
    "        \n",
    "        self.model = self.build_model(X_train.shape)\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train, \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size, \n",
    "            validation_split=0.2,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        loss, accuracy = self.model.evaluate(X_test, y_test)\n",
    "        print(f\"\\nTest Loss: {loss:.4f}\")\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        with open('tokenizer.pkl', 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "        print(\"Tokenizer saved successfully!\")\n",
    "        \n",
    "        return history\n",
    "\n",
    "def main():\n",
    "    fake_csv_path = 'Fake.csv'\n",
    "    true_csv_path = 'True.csv'\n",
    "    detector = FakeNewsDetector(max_features=10000, max_length=500)\n",
    "    detector.train(fake_csv_path, true_csv_path)\n",
    "    detector.model.save('fake_news_cnn_model.h5')\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PREDICTION SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Tokenizer loaded successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step\n",
      "\n",
      "Prediction Result:\n",
      "Fake News: Yes\n",
      "Confidence: 100.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\n",
      "Prediction Result:\n",
      "Fake News: No\n",
      "Confidence: 100.00%\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "class FakeNewsPredictor:\n",
    "    def __init__(self, model_path='fake_news_cnn_model.h5', tokenizer_path='tokenizer.pkl', max_length=500):\n",
    "        \"\"\"Initialize predictor with pre-trained model and tokenizer\"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.model = load_model(model_path)\n",
    "        print(\"Model loaded successfully!\")\n",
    "        \n",
    "        with open(tokenizer_path, 'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "        print(\"Tokenizer loaded successfully!\")\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocess input text\"\"\"\n",
    "        text = text.lower()\n",
    "        sequence = self.tokenizer.texts_to_sequences([text])\n",
    "        padded_sequence = pad_sequences(sequence, maxlen=self.max_length, truncating='post', padding='post')\n",
    "        return padded_sequence\n",
    "    \n",
    "    def predict_news(self, text):\n",
    "        \"\"\"Predict if news is fake or real\"\"\"\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        prediction = self.model.predict(processed_text)[0][0]\n",
    "        is_fake = prediction > 0.5\n",
    "        confidence = prediction if is_fake else 1 - prediction\n",
    "        return {\n",
    "            'is_fake': is_fake,\n",
    "            'confidence': float(confidence),\n",
    "            'fake_probability': float(prediction)\n",
    "        }\n",
    "    \n",
    "    # Add detailed preprocessing and prediction logging\n",
    "    def detailed_prediction(text):\n",
    "        processed_text = predictor.preprocess_text(text)\n",
    "        raw_prediction = predictor.model.predict(processed_text)[0][0]\n",
    "        \n",
    "        print(\"Raw Prediction Value:\", raw_prediction)\n",
    "        print(\"Processed Text Shape:\", processed_text.shape)\n",
    "    # Create predictor instance\n",
    "        predictor = FakeNewsPredictor()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = FakeNewsPredictor(\n",
    "        model_path='fake_news_cnn_model.h5', \n",
    "        tokenizer_path='tokenizer.pkl'\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        # Accept user input\n",
    "        text = input(\"\\nEnter news text to check (or type 'exit' to quit): \")\n",
    "        if text.lower() == 'exit':\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        \n",
    "        # Predict and display results\n",
    "        result = predictor.predict_news(text)\n",
    "        \n",
    "        print(\"\\nPrediction Result:\")\n",
    "        print(f\"Fake News: {'Yes' if result['is_fake'] else 'No'}\")\n",
    "        print(f\"Confidence: {result['confidence']*100:.2f}%\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Function to check news\n",
    "# def check_news(text):\n",
    "#     result = predictor.predict_news(text)\n",
    "#     print(\"\\nPrediction Result:\")\n",
    "#     print(f\"Fake News: {'Yes' if result['is_fake'] else 'No'}\")\n",
    "#     print(f\"Confidence: {result['confidence']*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
